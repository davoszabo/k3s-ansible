---

# K3S CLUSTER CONFIGURATION

k3s_servers:
  hosts:
    k3s-server-01:
      ansible_host: 192.168.0.10
      
k3s_workers:
  hosts:
    k3s-worker-01:
      ansible_host: 192.168.0.11
    k3s-worker-02:
      ansible_host: 192.168.0.12
      
k3s_cluster:
  children:
    k3s_servers:
    k3s_workers:
  vars:
    # The name of the cluster. Can be fiction, or anything.
    cluster_name: k3s_cluster
    
    # The k3s version.
    k3s_version: v1.31.3+k3s1
    
    # This is the user that has SSH access to the machines.
    ansible_user: root
    
    # The systemd directory.
    systemd_dir: /etc/systemd/system
    
    # Your timezone.
    system_timezone: Europe/Budapest
    
    # Token needed to ensure control planes can talk together securely. This token should be alpha numeric only!
    k3s_token: "SOME-SUPER-SECRET-PASSWORD"
    
    # Virtual ip-address which will be configured on each master.
    apiserver_endpoint: "192.168.0.20"
    
    # If you intended to use other storage provisioners like Longhorn, you should disable local-storage.
    local_storage_provisioner:
      # If set to false, no deployment will be initiated for local-storage, which saves space and power. 
      # Make sure to use other provisioner for storage if you set this to false!
      enabled: true
    
    # The type of load balancer to access services inside cluster.
    # -- Options: metallb | kube-vip
    load_balancer: "kube-vip"
    
    kube_vip:
      # Image tag for kube-vip.
      tag_version: v0.8.9

      # Tag for kube-vip-cloud-provider manifest.
      cloud_provider_tag_version: "main"
      
      # kube-vip IP range for load balancer.
      lb_ip_range: "192.168.0.21-192.168.0.29"
    
    # NOT SUPPORTED YET!
    metallb:
      # Metallb type frr or native.
      type: "native"
      
      # Metallb mode layer2 or bgp.
      mode: "layer2"
      bgp_my_asn: "64513"
      bgp_peer_asn: "64512"
      bgp_peer_address: "192.168.30.1"
      
      # Image tags for metal lb.
      speaker_tag_version: "v0.13.9"
      controller_tag_version: "v0.13.9"
      
      # Metallb ip range for load balancer.
      ip_range: "192.168.0.21-192.168.0.29"
    
    # Container Network Interface.
    # -- Supported options: flannel | cilium
    cni: "flannel"
    
    # If using calico or cilium, you may specify the cluster pod cidr pool!
    cluster_cidr: "10.42.0.0/16"
    
    flannel:
      # Interface which will be used for flannel.
      iface: "eth0"
    
    # NOT SUPPORTED YET!
    calico:
      # CALICO iface to use tigera operator/calico cni instead of flannel https://docs.tigera.io/calico/latest/about
      iface: "eth0"
      
      # use eBPF dataplane instead of iptables
      ebpf: false        
      
      # calico version tag
      tag: "v3.27.0"
    
    # cilium cni instead of flannel or calico
    # ensure v4.19.57, v5.1.16, v5.2.0 or more recent kernel
    cilium:
      iface: "eth0"
      
      # native when nodes on same subnet or using bgp, else set routed
      mode: "native"
      
      # cilium version tag
      tag: "v1.17.0"
      
      # enable hubble observability relay and ui
      hubble: true          
      
      # enable cilium bgp control plane for lb services and pod cidrs. disables metallb.
      bgp: false
      
      # bgp parameters for cilium cni. only active when iface is defined and bgp is true.
      bgp_my_asn: "64513"
      bgp_peer_asn: "64512"
      bgp_peer_address: "192.168.30.1"
      
      # cidr for cilium loadbalancer ipam
      bgp_lb_cidr: "192.168.31.0/24"   
    
    # --- EXTRA ARGUMENTS  ---
    
    # NOT SUPPORTED YET!
    # For the use of Rasberry PIs.
    raspberrypi:
      enabled: false
    
    # NOT SUPPORTED YET!
    # Only enable if your nodes are proxmox LXC nodes, make sure to configure your proxmox nodes
    # in your hosts.ini file.
    # Please read https://gist.github.com/triangletodd/02f595cd4c0dc9aac5f7763ca2264185 before using this.
    # Most notably, your containers must be privileged, and must not have nesting set to true.
    # Please note this script disables most of the security of lxc containers, with the trade off being that lxc
    # containers are significantly more resource efficient compared to full VMs.
    # Mixing and matching VMs and lxc containers is not supported, ymmv if you want to do this.
    # I would only really recommend using this if you have particularly low powered proxmox nodes where the overhead of
    # VMs would use a significant portion of your available resources.
    proxmox_lxc:
      enabled: false
      # the user that you would use to ssh into the host, for example if you run ssh some-user@my-proxmox-host,
      # set this value to some-user
      ssh_user: root
      # the unique proxmox ids for all of the containers in the cluster, both worker and master nodes
      ct_ids:
        - 200
        - 201
        - 202
        - 203
        - 204
    
    # NOT SUPPORTED YET!
    # Only enable this if you have set up your own container registry to act as a mirror / pull-through cache
    # (harbor / nexus / docker's official registry / etc).
    # Can be beneficial for larger dev/test environments (for example if you're getting rate limited by docker hub),
    # or air-gapped environments where your nodes don't have internet access after the initial setup
    # (which is still needed for downloading the k3s binary and such).
    # k3s's documentation about private registries here: https://docs.k3s.io/installation/private-registry
    custom_registries: 
      enabled: false
      
      # The registries can be authenticated or anonymous, depending on your registry server configuration.
      # If they allow anonymous access, simply remove the following bit from custom_registries_yaml
      #   configs:
      #     "registry.domain.com":
      #       auth:
      #         username: yourusername
      #         password: yourpassword
      # The following is an example that pulls all images used in this playbook through your private registries.
      # It also allows you to pull your own images from your private registry, without having to use imagePullSecrets
      # in your deployments.
      # If all you need is your own images and you don't care about caching the docker/quay/ghcr.io images,
      # you can just remove those from the mirrors: section.
      yaml_content: |
        mirrors:
          docker.io:
            endpoint:
              - "https://registry.domain.com/v2/dockerhub"
          quay.io:
            endpoint:
              - "https://registry.domain.com/v2/quayio"
          ghcr.io:
            endpoint:
              - "https://registry.domain.com/v2/ghcrio"
          registry.domain.com:
            endpoint:
              - "https://registry.domain.com"
              
        configs:
          "registry.domain.com":
            auth:
              username: yourusername
              password: yourpassword
    
    # NOT SUPPORTED YET!
    # Only enable and configure these if you access the internet through a proxy.
    proxy_env:
      enabled: false
      # HTTP_PROXY: "http://proxy.domain.local:3128"
      # HTTPS_PROXY: "http://proxy.domain.local:3128"
      # NO_PROXY: "*.domain.local,127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16"

# For 'terraform' commands

terraform:
  hosts:
    localhost:
  vars:
    state: "present"
    # -- Options: proxmox
    provider: proxmox
    proxmox: 
      api_url: "https://192.168.0.2:8006/api2/json"
      api_token_id: "terraform@pam!terraform-token"
      api_token_secret: ""
      target_node: "pve"
      # Start ID for VMs
      vmid: 1100
      template_name: "ubuntu24-cloudinit"
      gateway: "192.168.0.1"
      cpu_type: "x86-64-v3"
      vm_cores: 2
      vm_memory: 2048
      storage_pool: "local"
      storage_size: "4G"
      ciuser: root
      cipassword: password
      ssh_public_key: |
        ssh-rsa ASDASD root@host

# For 'kubectl' and 'helm' commands

kubernetes:
  hosts:
    localhost:
  vars:
    traefik:
      enabled: false
      namespace: iam
      chart_version: v34.1.0
      # IP should be in the MetalLB/kube-vip-cloud-provider range
      lb_ip: 192.168.0.21
      ingress_class: "traefik-private"
      domain_name: "example.com"
      dashboard:
        enabled: true
        # <subdomain_name>.example.com
        subdomain_name: "traefik"
        basicauth:
          enabled: false
          # Install apache2-utils
          # htpasswd -nb <username> <password> | openssl base64
          secret: "BASE64-SECRET"
    
    cert_manager:
      enabled: false
      namespace: iam
      chart_version: v1.16.2
      certificate:
        # -- Options: selfsigned | letsencrypt
        type: "letsencrypt"
        selfsigned:
          cert_name: "domain-ca"
          common_name: "domain.com"
          country_name: "US"
          organization_name: "Domain"
        letsencrypt:
          # -- Options: staging | production
          environment: "staging" 
      dns:
        # -- Options: duckdns | cloudflare | local
        provider: "cloudflare"
        local:
          dns_server: "192.168.0.1"
        duckdns:
          email: "user@email.com"
          token: ""
        cloudflare:
          email: "user@email.com"
          api_token:  ""

    cilium:
      hubble_ui:
        enabled: false
        namespace: kube-system
        subdomain_name: hubble

    # This utility cleans up pods that are not in Running state
    pod_cleaner:
      enabled: false
      namespace: kube-system
      cron: "0 22 * * *"
      remove_before_days: "3" # DO not use 0
    
    cloudflared:
      enabled: false
      namespace: cloudflared
      chart_version: 0.1.1
      tunnel_token: ""

    rancher:
      enabled: false
      namespace: cattle-system
      chart_version: 2.10.0
      subdomain_name: rancher
      bootstrapPassword: SOME-SUPER-SECRET-PASSWORD
    
    authentik:
      enabled: false
      namespace: iam
      chart_version: 2024.12.3
      # <subdomain_name>.example.com
      subdomain_name: authentik
      # If your dns_domain is "domain.com", it will be redirected to "<authentik.subdomain_name>.domain.com"
      redirect_domain_to_subdomain: true
      # Set your background URL/path or leave it empty "" to stick with defaults
      # "https://your-url-to-pic" or "/media/public/pic.jpg"
      # To disable background enter an invalid path like: "/media"
      custom_bg: ""
      secret_key: "SOME-SUPER-SECRET-PASSWORD"
      postgresql_password: "SOME-SUPER-SECRET-PASSWORD"
      admin:
        password: changeme
        email: user@example.org
      dashboard:
        enabled: true
      persistence:
        media:
          size: 2Gi
          existingClaim: ""
        database:
          size: 2Gi
          existingClaim: ""
        redis:
          size: 2Gi
          existingClaim: ""
    
    longhorn:
      enabled: false
      namespace: longhorn-system
      chart_version: 1.8.0
      dashboard:
        enabled: true
        # <subdomain_name>.example.com
        subdomain_name: "longhorn"
      # Enable minio to use it as S3 backup
      minio_backup:
        enabled: false
        key: ""
        secret: ""

    gitlab:
      # enabled: false
      # namespace: gitlab
      # chart_version: x.x.x
      runner:
        enabled: false
        namespace: gitlab
        chart_version: 0.67.0
        url: "https://gitlab.example.com"
        token: ""

    argocd:
      enabled: false
      namespace: argocd
      chart_version: 7.3.11
      dashboard:
        enabled: true
        subdomain_name: "argocd"
      oidc:
        enabled: false
        name: Authentik
        issuer: https://authentik.domain.com/application/o/argocd/
        clientID: ""
        clientSecret: ""
        requestedScopes:
          - openid
          - profile
          - email
        rbac:
          admin_group_name: authentik_admins
      image_updater:
        enabled: true
        namespace: argocd
        chart_version: 0.11.0
        registries:
          - name: GitLab Container Registry
            api_url: https://registry.gitlab.com
            prefix: gcr.io
            ping: no
            credentials: pullsecret:argocd/regcred
        private_registry:
          enabled: false
          # Each project (namespace) should have the same regcred, add them accordingly
          namespaces:
            - argocd
          # This is where images are pulled from
          secret:
            registry_name: "registry.gitlab.com/group/project"
            username: ""
            # Gather Personal Access Token with proper access
            token: ""
            email: ""

    nextcloud:
      enabled: false
      namespace: nextcloud
      chart_version: 4.6.8
      # <subdomain_name>.example.com
      subdomain_name: "nextcloud"
      admin_user:
        username: admin
        password: changeme
      data:
        # The worker node's mounted external drive
        path: /pool/nextcloud
        # The name of the worker node which has a mounted external drive (HDD or SSD)
        node_name: node-name
        # Size in Gi
        size: 200Gi
      config:
        # Size in Gi
        size: 3Gi
        # Use existing if present (won't create new PVCs)
        existingClaim: ""
      # Additional applications
      apps:
        # Enhances performance with pre generating thumbnails for images and videos in a separate folder.
        previewgenerator:
          enabled: true
          # IMPORTANT: Data need to be present and the command "php /var/www/html/occ preview:generate-all -vvv" have to be executed before starting the first cron job!
          # Previews for all images/videos will be generated into /nextcloud/data/appdata_*/previews
          # By default the pre generation of previews happens each day at 8 PM. The /cron.sh will be started! Logs are under /cron.log
          pre_generate_cron: "0 20 * * *"
      postgresql:
        enabled: true
        password: "SOME-SUPER-SECRET-PASSWORD"
        size: 4Gi
        existingClaim: ""
      redis:
        enabled: true
        password: "SOME-SUPER-SECRET-PASSWORD"
        size: 4Gi
        existingClaim: ""
        
    # Experimental only
    nfs_client:
      enabled: false
      namespace: nfs-client
      chart_version: 4.0.18
      server: 192.168.0.30
      path: /export/shared
      group_id: "100"

    qbittorrent:
      enabled: false
      namespace: qbittorrent
      chart_version: 0.3.7
      data:
        size: 500Gi
        # Use the GID that your users have access to!
        group_id: "100"
      config:
        size: 1Gi
        existingClaim: ""
      # Only NFS supported yet for storage provisioning
      # Install nfs utilities (utils or common libs) on the node which will mount with nfs!
      nfs:
        enabled: false
        server: 192.168.0.30
        path: /export/shared
      subdomain_name: "qbittorrent"
        
    prometheus:
      enabled: false
      namespace: monitoring
      chart_version: 63.1.0
      # Node Affinity to list of hostnames
      restrictToNodes:
        - k3s-bastion-worker-03
      grafana:
        enabled: true
        subdomain_name: "grafana"
        persistence:
          enabled: true
          size: 3Gi
          existingClaim: ""
        oidc:
          enabled: false
          name: authentik
          oauth_auto_login: true
          client_id: ""
          client_secret: ""
          scopes: "openid profile email"
          auth_url: ""
          token_url: ""
          api_url: ""
          signout_redirect_url: ""
          role_attribute_path: contains(groups, 'authentik_admins') && 'Admin' || contains(groups, 'developers') && 'Editor' || 'Viewer'
      loki:
        # ENABLE ONLY IF MINIO IS SET UP PROPERLY
        enabled: false
        subdomain_name: "loki"
        chart_version: 6.22.0
        persistence:
          enabled: true
          size: 3Gi
          existingClaim: ""
        s3:
          access_key: ""
          secret: ""
          region: ""
          endpoint: "https://{{ minio.subdomain_name + '-api.' + traefik.domain_name }}"
          bucket_name: loki

    minio:
      enabled: false
      namespace: minio
      chart_version: 14.7.13
      # This is for WebUI
      subdomain_name: "minio"
      # To access the bucket via API the endpoint will be the "subdomain_name"-api.domain.com
      admin_user:
        username: minioadmin
        password: "SOME-SUPER-SECRET-PASSWORD"
      # Changes file and folder permissions for the mount. In case it takes too much time, it can be disabled, but make sure it ran at least once.
      volumePermissions: true
      data:
        # The worker node's mounted external drive
        path: /pool/minio
        # The name of the worker node which has a mounted external drive (HDD or SSD)
        node_name: node-name
        # Size in Gi
        size: 200Gi
      oidc:
        enabled: false

    home_assistant:
      enabled: false
      namespace: home-assistant
      chart_version: 0.2.82
      # This is for WebUI
      subdomain_name: "home-assistant"
      persistence:
        size: 3Gi
        existingVolume: ""
      oidc:
        enabled: false

    valheim:
      enabled: false
      namespace: valheim
      chart_version: 2.0.1
      # leave blank for new world (Dedicated) or use existing one
      world_name: ""
      server_name: valheim-server
      password: superSecurePW123
      # -- Options: "" (for default) | local-storage
      storage_class: local-storage
      local_storage:
        # The worker node's mounted external drive
        path: /pool/valheim
        # The name of the worker node which has a mounted external drive (HDD or SSD)
        node_name: node-name
      data:
        # Size in Gi
        world:
          size: 1Gi
        server:
          size: 5Gi

    it_tools:
      enabled: false
      namespace: it-tools
      chart_version: 0.1.1
      # This is for WebUI
      subdomain_name: "it-tools"

